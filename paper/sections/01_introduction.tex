\section{Introduction}

Algorithmic pricing systems increasingly rely on autonomous reinforcement learning (RL) agents to adapt prices in competitive markets. Recent evidence demonstrates that independent RL agents can converge to supra-competitive outcomes resembling tacit collusion, even in the absence of communication or explicit coordination~\cite{calvano2020artificial}. Such outcomes raise serious regulatory and economic concerns, as coordinated pricing harms consumer welfare and undermines market competition.

While a growing literature documents the emergence of algorithmic collusion, far less attention has been given to its persistence once learned. In real-world markets, regulatory interventions occur only after harmful behavior has been detected; therefore, understanding whether learned collusion is fragile or self-reinforcing is of central importance for antitrust policy. Previous studies have shown that collusion \emph{can} emerge, but they have not systematically tested whether it \emph{can be disrupted} once established.

This paper investigates the stability of tacit collusion formed by independent Q-learning agents in repeated Bertrand competition and evaluates the effectiveness of regulatory-style interventions designed to disrupt such behavior. We examine four intervention types: forced competitive pricing (simulating regulatory fines), exploration shocks (simulating market audits), memory resets (simulating algorithmic updates), and their effects on collusion persistence.

\textbf{Research Questions.}
\begin{itemize}
    \item \textbf{RQ1:} How stable is algorithmic collusion learned by independent Q-learning agents to various market disruptions?
    \item \textbf{RQ2:} What are the effects of different intervention types on collusive equilibria?
    \item \textbf{RQ3:} Do interventions successfully disrupt coordination, or do they paradoxically reinforce it?
\end{itemize}

\textbf{Key Findings.} Our simulations reveal a troubling paradox: rather than disrupting collusion, interventions consistently \emph{strengthen} it. Independent Q-learning agents converged to an average price of 1.561 (30\% toward monopoly pricing from the Nash equilibrium). After interventions:
\begin{itemize}
    \item Forced competitive pricing increased prices by 3.6-3.7\%
    \item Exploration shocks increased prices by 2.2\%
    \item Memory resets increased prices by 3.4\%
\end{itemize}
All interventions yielded recovery rates exceeding 110\%, indicating a \emph{collusion reinforcement effect} where disruptions lead to more robust coordination.

\textbf{Contributions.}
\begin{itemize}
    \item First systematic analysis of algorithmic collusion stability under multiple intervention types
    \item Identification of a \emph{collusion reinforcement paradox} where interventions strengthen rather than weaken coordination
    \item Empirical evidence that traditional regulatory tools may be inadequate or counterproductive for algorithmic markets
    \item Policy implications for antitrust regulation in increasingly automated markets
\end{itemize}

Our findings suggest that algorithmic collusion exhibits self-reinforcing properties, challenging conventional antitrust wisdom and highlighting the need for preventive rather than reactive regulatory approaches.